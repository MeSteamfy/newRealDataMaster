{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299837f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TPs/credit_scoring.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     30\u001b[39m mlp_base_params = {\u001b[33m'\u001b[39m\u001b[33mhidden_layer_sizes\u001b[39m\u001b[33m'\u001b[39m: (\u001b[32m40\u001b[39m, \u001b[32m20\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mmax_iter\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1000\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrandom_state\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m}\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# PARTIE I : APPRENTISSAGE SUPERVISÉ - credit_scoring.csv\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Chargement des données (Assurez-vous que le chemin TPs/ est correct)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTPs/credit_scoring.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Données \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcredit_scoring.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m chargées.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Transformer en numpy Array et séparer X et Y\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iiomq\\OneDrive\\Documents\\Cours\\Université\\M1\\1er semestre\\App et Analyse data\\TPs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iiomq\\OneDrive\\Documents\\Cours\\Université\\M1\\1er semestre\\App et Analyse data\\TPs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iiomq\\OneDrive\\Documents\\Cours\\Université\\M1\\1er semestre\\App et Analyse data\\TPs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iiomq\\OneDrive\\Documents\\Cours\\Université\\M1\\1er semestre\\App et Analyse data\\TPs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\iiomq\\OneDrive\\Documents\\Cours\\Université\\M1\\1er semestre\\App et Analyse data\\TPs\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'TPs/credit_scoring.csv'"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Algorithmes et Analyse de données - Atelier Apprentissage Supervisé\n",
    "# ====================================================================\n",
    "\n",
    "# --- Configuration et Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Importez votre fichier utilitaire\n",
    "import utils \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=10000, suppress=True)\n",
    "\n",
    "# Définition des classifieurs initiaux (Q2)\n",
    "classifiers_q2 = {\n",
    "    'CART': DecisionTreeClassifier(random_state=1),\n",
    "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'MLP (40-20)': MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1, max_iter=1000)\n",
    "}\n",
    "mlp_base_params = {'hidden_layer_sizes': (40, 20), 'max_iter': 1000, 'random_state': 1}\n",
    "\n",
    "# ====================================================================\n",
    "# PARTIE I : APPRENTISSAGE SUPERVISÉ - credit_scoring.csv\n",
    "# ====================================================================\n",
    "\n",
    "## Question 1: Chargement des données et préparation\n",
    "\n",
    "# Chargement des données (Assurez-vous que le chemin TPs/ est correct)\n",
    "df = pd.read_csv(\"./credit_scoring.csv\", sep=';', engine='python')\n",
    "print(\"✅ Données 'credit_scoring.csv' chargées.\")\n",
    "\n",
    "# Transformer en numpy Array et séparer X et Y\n",
    "X = df.values[:, :-1]  # Variables caractéristiques\n",
    "Y = df.values[:, -1]   # Variable à prédire (Status)\n",
    "\n",
    "print(f\"Taille de l'échantillon complet: {X.shape[0]} individus et {X.shape[1]} variables\")\n",
    "\n",
    "# Séparer les données en apprentissage (50%) et test (50%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=1\n",
    ")\n",
    "\n",
    "print(f\"✅ Séparation Train/Test (50/50) effectuée. Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "## Question 2: Apprentissage et évaluation de modèles (Sans Prétraitement)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 2: COMPARAISON AVEC DONNÉES BRUTES\")\n",
    "print(\"=\"*80)\n",
    "meilleur_nom_q2, meilleur_modele_q2 = utils.run_classifiers_train_test(\n",
    "    classifiers_q2, X_train, Y_train, X_test, Y_test, mode='Brut'\n",
    ")\n",
    "\n",
    "## Question 3: Normalisation des variables continues\n",
    "\n",
    "# Normaliser les données \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✅ Données standardisées (StandardScaler).\")\n",
    "\n",
    "# Exécuter à nouveau les classifieurs sur les données normalisées\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 3: COMPARAISON AVEC DONNÉES NORMALISÉES\")\n",
    "print(\"=\"*80)\n",
    "meilleur_nom_q3, meilleur_modele_q3 = utils.run_classifiers_train_test(\n",
    "    classifiers_q2, X_train_scaled, Y_train, X_test_scaled, Y_test, mode='Normalisé'\n",
    ")\n",
    "\n",
    "## Question 4: Création de nouvelles variables (ACP)\n",
    "\n",
    "# Application de l'ACP (sur données déjà scalées)\n",
    "pca = PCA(n_components=3, random_state=1)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Concaténer les données originales scalées avec les 3 composantes principales\n",
    "X_train_with_pca = np.concatenate([X_train_scaled, X_train_pca], axis=1)\n",
    "X_test_with_pca = np.concatenate([X_test_scaled, X_test_pca], axis=1)\n",
    "\n",
    "print(f\"✅ ACP terminée. Dimensions après concaténation: {X_train_with_pca.shape}\")\n",
    "print(f\"Variance totale expliquée par les 3 PC: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# Exécuter à nouveau les classifieurs sur les données normalisées + ACP\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 4: COMPARAISON AVEC DONNÉES NORMALISÉES + ACP\")\n",
    "print(\"=\"*80)\n",
    "meilleur_nom_q4, meilleur_modele_q4 = utils.run_classifiers_train_test(\n",
    "    classifiers_q2, X_train_with_pca, Y_train, X_test_with_pca, Y_test, mode='Normalisé + ACP'\n",
    ")\n",
    "\n",
    "## Question 5: Sélection de variables (K Optimal)\n",
    "\n",
    "# Définir la liste des noms de colonnes (13 originales + 3 PCA)\n",
    "nom_cols_pca = list(df.columns[:-1]) + ['PC1', 'PC2', 'PC3']\n",
    "\n",
    "# 1. Importance des variables (Q5 - Partie 1)\n",
    "sorted_idx, feature_names = utils.importance_des_variables(X_train_with_pca, Y_train, nom_cols_pca)\n",
    "\n",
    "# 2. Sélection du nombre optimal de variables (Q5 - Partie 2)\n",
    "optimal_features_count = utils.selection_nombre_optimal_variables(\n",
    "    X_train_with_pca, X_test_with_pca, Y_train, Y_test, sorted_idx, meilleur_nom_q4, mlp_base_params\n",
    ")\n",
    "\n",
    "# Stocker les K indices sélectionnés pour la Q6 et Q7\n",
    "best_k_indices = sorted_idx[:optimal_features_count] \n",
    "print(f\"Le nombre optimal de variables à conserver est : {optimal_features_count}\")\n",
    "\n",
    "# 3. Explicabilité avec SHAP (Q5 - Partie 3)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 5 (Partie 3): EXPLICABILITÉ AVEC SHAP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Préparer les données avec les K meilleures variables\n",
    "X_train_best_k = X_train_with_pca[:, best_k_indices]\n",
    "X_test_best_k = X_test_with_pca[:, best_k_indices]\n",
    "best_feature_names = [nom_cols_pca[i] for i in best_k_indices]\n",
    "\n",
    "# Réentraîner le meilleur modèle de Q4 sur ces K variables\n",
    "if meilleur_nom_q4 == 'MLP (40-20)':\n",
    "    model_for_shap = MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1, max_iter=1000)\n",
    "elif meilleur_nom_q4 == 'KNN (k=5)':\n",
    "    model_for_shap = KNeighborsClassifier(n_neighbors=5)\n",
    "else:  # CART\n",
    "    model_for_shap = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "model_for_shap.fit(X_train_best_k, Y_train)\n",
    "print(f\"✅ Modèle {meilleur_nom_q4} entraîné sur les {optimal_features_count} meilleures variables.\")\n",
    "\n",
    "# Appliquer l'analyse SHAP\n",
    "shap_values = utils.explicabilite_shap(\n",
    "    model_for_shap, X_train_best_k, X_test_best_k, best_feature_names\n",
    ")\n",
    "\n",
    "## Question 6: Paramétrage des classifieurs (Tuning du MLP)\n",
    "\n",
    "# Préparer le jeu de données avec SEULEMENT les K variables optimales\n",
    "X_train_selected_for_tuning = X_train_with_pca[:, best_k_indices]\n",
    "\n",
    "# Définition de la grille de tuning pour le MLP (Q6)\n",
    "base_mlp = MLPClassifier(random_state=1, max_iter=1000)\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(30, 15), (40, 20), (50, 25)], \n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'alpha': [0.0001, 0.001, 0.01] \n",
    "}\n",
    "\n",
    "# Exécuter la recherche GridSearchCV\n",
    "tuned_mlp, best_mlp_params = utils.recherche_meilleurs_parametres(\n",
    "    X_train_selected_for_tuning, Y_train, base_mlp, param_grid_mlp\n",
    ")\n",
    "\n",
    "## Question 7 & 8: Création du Pipeline et Orchestration\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"ORCHESTRATION DU PIPELINE (Q1 à Q8) : Création du Pipeline Final\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# L'orchestration exécute Q1->Q7 et sauvegarde le pipeline\n",
    "final_pipeline_orchestre = utils.pipeline_generation_train_test_split(\n",
    "    df, X_train, Y_train, X_test, Y_test\n",
    ")\n",
    "\n",
    "# Validation Finale sur le jeu de test\n",
    "Y_pred_pipe = final_pipeline_orchestre.predict(X_test)\n",
    "Y_proba_pipe = final_pipeline_orchestre.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n--- Validation Finale du Pipeline Orchestré ---\")\n",
    "print(f\"Accuracy: {accuracy_score(Y_test, Y_pred_pipe):.4f}\")\n",
    "print(f\"AUC: {roc_auc_score(Y_test, Y_proba_pipe):.4f}\")\n",
    "print(f\"\\n✅ Pipeline de l'API (Q9) sauvegardé sous 'credit_scoring_pipeline.pkl'.\")\n",
    "\n",
    "\n",
    "## Question 10: Comparaison avec Validation Croisée (CV) et Orchestration Finale\n",
    "\n",
    "# 1. Comparaison par CV (sur X et Y bruts complets)\n",
    "X_data_q10 = df.iloc[:, :-1].values\n",
    "Y_data_q10 = df.iloc[:, -1].values\n",
    "\n",
    "best_clf_cv_name, best_ds_cv_name = utils.run_classifiers_cv(X_data_q10, Y_data_q10)\n",
    "\n",
    "# --- 2. Tuning et Orchestration finale (sur le meilleur CV) ---\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Si le modèle est Random Forest (meilleur probable), on utilise cette grille de tuning\n",
    "if best_clf_cv_name.startswith('Random Forest'):\n",
    "    base_clf_q10 = RandomForestClassifier(random_state=1, n_estimators=200, n_jobs=-1)\n",
    "    param_grid_q10 = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    \n",
    "    # Préparer les données d'entraînement complètes (pour le tuning final)\n",
    "    indices_train = train_test_split(range(len(X_data_q10)), test_size=0.5, random_state=1)[0]\n",
    "    X_train_full = X_data_q10[indices_train]\n",
    "    Y_train_full = Y_data_q10[indices_train]\n",
    "    X_scaled_full = StandardScaler().fit_transform(X_train_full)\n",
    "    X_pca_full = np.concatenate([X_scaled_full, PCA(n_components=3, random_state=1).fit_transform(X_scaled_full)], axis=1)\n",
    "    \n",
    "    tuned_final_clf, best_cv_params = utils.recherche_meilleurs_parametres(\n",
    "        X_pca_full[:, best_k_indices], Y_train_full, base_clf_q10, param_grid_q10\n",
    "    )\n",
    "else:\n",
    "    tuned_final_clf = tuned_mlp\n",
    "    best_cv_params = best_mlp_params\n",
    "\n",
    "# 3. Orchestration Finale CV (Entraînement sur TOUTES les données X_data_q10)\n",
    "final_pipeline_cv_orchestre = utils.pipeline_generation_cv(\n",
    "    df, X_data_q10, Y_data_q10, best_clf_cv_name, best_cv_params, optimal_features_count, X_test, Y_test\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Partie I (Q1-Q10) complète. Le pipeline de production est sauvegardé sous 'final_production_pipeline_cv.pkl'.\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PARTIE II : DONNÉES HÉTÉROGÈNES - credit.data\n",
    "# ====================================================================\n",
    "\n",
    "## Question II.1: Données numériques seules\n",
    "\n",
    "# Définir les colonnes numériques (indices 0 à 5) et catégorielles (6 à 14) (simulé selon le TP)\n",
    "col_num = np.arange(6)\n",
    "col_cat = np.arange(6, 15)\n",
    "\n",
    "try:\n",
    "    # Lecture du fichier credit.data (qui utilise '?' pour les valeurs manquantes)\n",
    "    df_credit_data = pd.read_csv(\"./credit.data\", header=None, na_values='?', sep=',')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n⚠️ Fichier 'credit.data' non trouvé. Simulation avec les premières colonnes de 'credit_scoring.csv'.\")\n",
    "    df_credit_data = df.copy() \n",
    "    col_num = np.arange(6)\n",
    "    col_cat = np.arange(6, 13)\n",
    "\n",
    "# 1. Préparation des données numériques\n",
    "X_train_num, X_test_num, Y_train_num, Y_test_num = utils.traitement_donnees_numeriques(df_credit_data, col_num)\n",
    "\n",
    "# Exécuter run_classifiers (sans normalisation)\n",
    "classifiers_ii_1 = {'CART': DecisionTreeClassifier(random_state=1), 'KNN (k=5)': KNeighborsClassifier(n_neighbors=5)}\n",
    "utils.run_classifiers_train_test(classifiers_ii_1, X_train_num, Y_train_num, X_test_num, Y_test_num, mode='II.1 Brut')\n",
    "\n",
    "# 2. Normalisation et exécution\n",
    "scaler_ii = StandardScaler()\n",
    "X_train_num_scaled = scaler_ii.fit_transform(X_train_num)\n",
    "X_test_num_scaled = scaler_ii.transform(X_test_num)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION II.1 (Suite): COMPARAISON AVEC DONNÉES NORMALISÉES\")\n",
    "print(\"=\"*80)\n",
    "utils.run_classifiers_train_test(classifiers_ii_1, X_train_num_scaled, Y_train_num, X_test_num_scaled, Y_test_num, mode='II.1 Normalisé')\n",
    "\n",
    "## Question II.2: Données hétérogènes (avec imputation)\n",
    "\n",
    "# Recharger les données complètes pour l'imputation (X_full, Y_full)\n",
    "X_full_ii = df_credit_data.values[:, :-1]\n",
    "Y_full_ii = df_credit_data.values[:, -1]\n",
    "\n",
    "# Traitement de données hétérogènes par imputation et One-Hot Encoding\n",
    "X_train_final_ii, X_test_final_ii, Y_train_comb_ii, Y_test_comb_ii = utils.traitement_donnees_heterogenes_imputation(\n",
    "    X_full_ii, Y_full_ii, col_num, col_cat\n",
    ")\n",
    "\n",
    "# Exécuter run_classifiers sur les nouvelles données (Q6)\n",
    "classifiers_ii_2 = {'CART': DecisionTreeClassifier(random_state=1), 'MLP (20-10)': MLPClassifier(hidden_layer_sizes=(20, 10), random_state=1, max_iter=500)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION II.2: COMPARAISON SUR DONNÉES IMPUTÉES/NORMALISÉES/OHE\")\n",
    "print(\"=\"*80)\n",
    "utils.run_classifiers_train_test(classifiers_ii_2, X_train_final_ii, Y_train_comb_ii, X_test_final_ii, Y_test_comb_ii, mode='II.2 Final')\n",
    "\n",
    "print(\"\\n✅ Partie II complète.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
