{
 "cells": [
  {
   "cell_type": "code",
   "id": "299837f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:35:24.950091Z",
     "start_time": "2025-12-11T13:35:20.321342Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# ====================================================================\n",
    "# Algorithmes et Analyse de données - Atelier Apprentissage Supervisé\n",
    "# ====================================================================\n",
    "\n",
    "# --- Configuration et Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Importez votre fichier utilitaire\n",
    "import utils \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=10000, suppress=True)\n",
    "\n",
    "# Définition des classifieurs initiaux (Q2)\n",
    "classifiers_q2 = {\n",
    "    'CART': DecisionTreeClassifier(random_state=1),\n",
    "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'MLP (40-20)': MLPClassifier(hidden_layer_sizes=(40, 20), random_state=1, max_iter=1000)\n",
    "}\n",
    "mlp_base_params = {'hidden_layer_sizes': (40, 20), 'max_iter': 1000, 'random_state': 1}\n",
    "\n",
    "# ====================================================================\n",
    "# PARTIE I : APPRENTISSAGE SUPERVISÉ - credit_scoring.csv\n",
    "# ====================================================================\n",
    "\n",
    "## Question 1: Chargement des données et préparation\n",
    "\n",
    "# Chargement des données (Assurez-vous que le chemin TPs/ est correct)\n",
    "df = pd.read_csv(\"./credit_scoring.csv\", sep=';', engine='python')\n",
    "print(\"✅ Données 'credit_scoring.csv' chargées.\")\n",
    "\n",
    "# Transformer en numpy Array et séparer X et Y\n",
    "X = df.values[:, :-1]  # Variables caractéristiques\n",
    "Y = df.values[:, -1]   # Variable à prédire (Status)\n",
    "\n",
    "print(f\"Taille de l'échantillon complet: {X.shape[0]} individus et {X.shape[1]} variables\")\n",
    "\n",
    "# Séparer les données en apprentissage (50%) et test (50%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=1\n",
    ")\n",
    "\n",
    "print(f\"✅ Séparation Train/Test (50/50) effectuée. Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "## Question 2: Apprentissage et évaluation de modèles (Sans Prétraitement)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 2: COMPARAISON AVEC DONNÉES BRUTES\")\n",
    "print(\"=\"*80)\n",
    "meilleur_nom_q2, meilleur_modele_q2 = utils.run_classifiers_train_test(\n",
    "    classifiers_q2, X_train, Y_train, X_test, Y_test, mode='Brut'\n",
    ")\n",
    "\n",
    "## Question 3: Normalisation des variables continues\n",
    "\n",
    "# Normaliser les données \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✅ Données standardisées (StandardScaler).\")\n",
    "\n",
    "# Exécuter à nouveau les classifieurs sur les données normalisées\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 3: COMPARAISON AVEC DONNÉES NORMALISÉES\")\n",
    "print(\"=\"*80)\n",
    "meilleur_nom_q3, meilleur_modele_q3 = utils.run_classifiers_train_test(\n",
    "    classifiers_q2, X_train_scaled, Y_train, X_test_scaled, Y_test, mode='Normalisé'\n",
    ")\n",
    "\n",
    "## Question 4: Création de nouvelles variables (ACP)\n",
    "\n",
    "# Application de l'ACP (sur données déjà scalées)\n",
    "pca = PCA(n_components=3, random_state=1)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Concaténer les données originales scalées avec les 3 composantes principales\n",
    "X_train_with_pca = np.concatenate([X_train_scaled, X_train_pca], axis=1)\n",
    "X_test_with_pca = np.concatenate([X_test_scaled, X_test_pca], axis=1)\n",
    "\n",
    "print(f\"✅ ACP terminée. Dimensions après concaténation: {X_train_with_pca.shape}\")\n",
    "print(f\"Variance totale expliquée par les 3 PC: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# Exécuter à nouveau les classifieurs sur les données normalisées + ACP\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 4: COMPARAISON AVEC DONNÉES NORMALISÉES + ACP\")\n",
    "print(\"=\"*80)\n",
    "meilleur_nom_q4, meilleur_modele_q4 = utils.run_classifiers_train_test(\n",
    "    classifiers_q2, X_train_with_pca, Y_train, X_test_with_pca, Y_test, mode='Normalisé + ACP'\n",
    ")\n",
    "\n",
    "## Question 5: Sélection de variables (K Optimal)\n",
    "\n",
    "# Définir la liste des noms de colonnes (13 originales + 3 PCA)\n",
    "nom_cols_pca = list(df.columns[:-1]) + ['PC1', 'PC2', 'PC3']\n",
    "\n",
    "# 1. Importance des variables (Q5 - Partie 1)\n",
    "sorted_idx, feature_names = utils.importance_des_variables(X_train_with_pca, Y_train, nom_cols_pca)\n",
    "\n",
    "# 2. Sélection du nombre optimal de variables (Q5 - Partie 2)\n",
    "optimal_features_count = utils.selection_nombre_optimal_variables(\n",
    "    X_train_with_pca, X_test_with_pca, Y_train, Y_test, sorted_idx, meilleur_nom_q4, mlp_base_params\n",
    ")\n",
    "\n",
    "# Stocker les K indices sélectionnés pour la Q6 et Q7\n",
    "best_k_indices = sorted_idx[:optimal_features_count] \n",
    "print(f\"Le nombre optimal de variables à conserver est : {optimal_features_count}\")\n",
    "\n",
    "## Question 6: Paramétrage des classifieurs (Tuning du MLP)\n",
    "\n",
    "# Préparer le jeu de données avec SEULEMENT les K variables optimales\n",
    "X_train_selected_for_tuning = X_train_with_pca[:, best_k_indices]\n",
    "\n",
    "# Définition de la grille de tuning pour le MLP (Q6)\n",
    "base_mlp = MLPClassifier(random_state=1, max_iter=1000)\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(30, 15), (40, 20), (50, 25)], \n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'alpha': [0.0001, 0.001, 0.01] \n",
    "}\n",
    "\n",
    "# Exécuter la recherche GridSearchCV\n",
    "tuned_mlp, best_mlp_params = utils.recherche_meilleurs_parametres(\n",
    "    X_train_selected_for_tuning, Y_train, base_mlp, param_grid_mlp\n",
    ")\n",
    "\n",
    "## Question 7 & 8: Création du Pipeline et Orchestration\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"ORCHESTRATION DU PIPELINE (Q1 à Q8) : Création du Pipeline Final\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# L'orchestration exécute Q1->Q7 et sauvegarde le pipeline\n",
    "final_pipeline_orchestre = utils.pipeline_generation_train_test_split(\n",
    "    df, X_train, Y_train, X_test, Y_test\n",
    ")\n",
    "\n",
    "# Validation Finale sur le jeu de test\n",
    "Y_pred_pipe = final_pipeline_orchestre.predict(X_test)\n",
    "Y_proba_pipe = final_pipeline_orchestre.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n--- Validation Finale du Pipeline Orchestré ---\")\n",
    "print(f\"Accuracy: {accuracy_score(Y_test, Y_pred_pipe):.4f}\")\n",
    "print(f\"AUC: {roc_auc_score(Y_test, Y_proba_pipe):.4f}\")\n",
    "print(f\"\\n✅ Pipeline de l'API (Q9) sauvegardé sous 'credit_scoring_pipeline.pkl'.\")\n",
    "\n",
    "\n",
    "## Question 10: Comparaison avec Validation Croisée (CV) et Orchestration Finale\n",
    "\n",
    "# 1. Comparaison par CV (sur X et Y bruts complets)\n",
    "X_data_q10 = df.iloc[:, :-1].values\n",
    "Y_data_q10 = df.iloc[:, -1].values\n",
    "\n",
    "best_clf_cv_name, best_ds_cv_name = utils.run_classifiers_cv(X_data_q10, Y_data_q10)\n",
    "\n",
    "# --- 2. Tuning et Orchestration finale (sur le meilleur CV) ---\n",
    "# Si le modèle est Random Forest (meilleur probable), on utilise cette grille de tuning\n",
    "if best_clf_cv_name.startswith('Random Forest'):\n",
    "    base_clf_q10 = RandomForestClassifier(random_state=1, n_estimators=200, n_jobs=-1)\n",
    "    param_grid_q10 = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    \n",
    "    # Préparer les données d'entraînement complètes (pour le tuning final)\n",
    "    X_train_full = X_data_q10[train_test_split(range(len(X_data_q10)), test_size=0.5, random_state=1)[0]]\n",
    "    Y_train_full = Y_data_q10[train_test_split(range(len(Y_data_q10)), test_size=0.5, random_state=1)[0]]\n",
    "    X_scaled_full = StandardScaler().fit_transform(X_train_full)\n",
    "    X_pca_full = np.concatenate([X_scaled_full, PCA(n_components=3, random_state=1).fit_transform(X_scaled_full)], axis=1)\n",
    "    \n",
    "    tuned_final_clf, best_cv_params = utils.recherche_meilleurs_parametres(\n",
    "        X_pca_full[:, best_k_indices], Y_train_full, base_clf_q10, param_grid_q10\n",
    "    )\n",
    "else:\n",
    "    tuned_final_clf = tuned_mlp\n",
    "    best_cv_params = best_mlp_params\n",
    "\n",
    "# 3. Orchestration Finale CV (Entraînement sur TOUTES les données X_data_q10)\n",
    "final_pipeline_cv_orchestre = utils.pipeline_generation_cv(\n",
    "    df, X_data_q10, Y_data_q10, best_clf_cv_name, best_cv_params, optimal_features_count, X_test, Y_test\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Partie I (Q1-Q10) complète. Le pipeline de production est sauvegardé sous 'final_production_pipeline_cv.pkl'.\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PARTIE II : DONNÉES HÉTÉROGÈNES - credit.data\n",
    "# ====================================================================\n",
    "\n",
    "## Question II.1: Données numériques seules\n",
    "\n",
    "# Définir les colonnes numériques (indices 0 à 5) et catégorielles (6 à 14) (simulé selon le TP)\n",
    "col_num = np.arange(6)\n",
    "col_cat = np.arange(6, 15)\n",
    "\n",
    "try:\n",
    "    # Lecture du fichier credit.data (qui utilise '?' pour les valeurs manquantes)\n",
    "    df_credit_data = pd.read_csv(\"./credit.data\", header=None, na_values='?', sep=',')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n⚠️ Fichier 'credit.data' non trouvé. Simulation avec les premières colonnes de 'credit_scoring.csv'.\")\n",
    "    df_credit_data = df.copy() \n",
    "    col_num = np.arange(6)\n",
    "    col_cat = np.arange(6, 13)\n",
    "\n",
    "# 1. Préparation des données numériques\n",
    "X_train_num, X_test_num, Y_train_num, Y_test_num = utils.traitement_donnees_numeriques(df_credit_data, col_num)\n",
    "\n",
    "# Exécuter run_classifiers (sans normalisation)\n",
    "classifiers_ii_1 = {'CART': DecisionTreeClassifier(random_state=1), 'KNN (k=5)': KNeighborsClassifier(n_neighbors=5)}\n",
    "utils.run_classifiers_train_test(classifiers_ii_1, X_train_num, Y_train_num, X_test_num, Y_test_num, mode='II.1 Brut')\n",
    "\n",
    "# 2. Normalisation et exécution\n",
    "scaler_ii = StandardScaler()\n",
    "X_train_num_scaled = scaler_ii.fit_transform(X_train_num)\n",
    "X_test_num_scaled = scaler_ii.transform(X_test_num)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION II.1 (Suite): COMPARAISON AVEC DONNÉES NORMALISÉES\")\n",
    "print(\"=\"*80)\n",
    "utils.run_classifiers_train_test(classifiers_ii_1, X_train_num_scaled, Y_train_num, X_test_num_scaled, Y_test_num, mode='II.1 Normalisé')\n",
    "\n",
    "## Question II.2: Données hétérogènes (avec imputation)\n",
    "\n",
    "# Recharger les données complètes pour l'imputation (X_full, Y_full)\n",
    "X_full_ii = df_credit_data.values[:, :-1]\n",
    "Y_full_ii = df_credit_data.values[:, -1]\n",
    "\n",
    "# Traitement de données hétérogènes par imputation et One-Hot Encoding\n",
    "X_train_final_ii, X_test_final_ii, Y_train_comb_ii, Y_test_comb_ii = utils.traitement_donnees_heterogenes_imputation(\n",
    "    X_full_ii, Y_full_ii, col_num, col_cat\n",
    ")\n",
    "\n",
    "# Exécuter run_classifiers sur les nouvelles données (Q6)\n",
    "classifiers_ii_2 = {'CART': DecisionTreeClassifier(random_state=1), 'MLP (20-10)': MLPClassifier(hidden_layer_sizes=(20, 10), random_state=1, max_iter=500)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION II.2: COMPARAISON SUR DONNÉES IMPUTÉES/NORMALISÉES/OHE\")\n",
    "print(\"=\"*80)\n",
    "utils.run_classifiers_train_test(classifiers_ii_2, X_train_final_ii, Y_train_comb_ii, X_test_final_ii, Y_test_comb_ii, mode='II.2 Final')\n",
    "\n",
    "print(\"\\n✅ Partie II complète.\")"
   ],
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/maxmbey/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/maxmbey/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mXGBoostError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mneural_network\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MLPClassifier\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Importez votre fichier utilitaire\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mutils\u001B[39;00m \n\u001B[32m     21\u001B[39m warnings.filterwarnings(\u001B[33m'\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     22\u001B[39m np.set_printoptions(threshold=\u001B[32m10000\u001B[39m, suppress=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Ecole/M1/Data/newRealDataMaster/utils.py:22\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mneural_network\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MLPClassifier\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mensemble\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mxgboost\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m XGBClassifier\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpipeline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     25\u001B[39m     confusion_matrix,\n\u001B[32m     26\u001B[39m     accuracy_score,\n\u001B[32m   (...)\u001B[39m\u001B[32m     31\u001B[39m     make_scorer\n\u001B[32m     32\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/__init__.py:6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001B[39;00m\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m \u001B[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tracker  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m collective, dask\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      9\u001B[39m     Booster,\n\u001B[32m     10\u001B[39m     DataIter,\n\u001B[32m   (...)\u001B[39m\u001B[32m     15\u001B[39m     build_info,\n\u001B[32m     16\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/tracker.py:9\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01menum\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m IntEnum, unique\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dict, Optional, Union\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _LIB, _check_call, make_jcargs\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_family\u001B[39m(addr: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mint\u001B[39m:\n\u001B[32m     13\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Get network family from address.\"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/core.py:269\u001B[39m\n\u001B[32m    265\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\n\u001B[32m    268\u001B[39m \u001B[38;5;66;03m# load the XGBoost library globally\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m _LIB = \u001B[43m_load_lib\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    272\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_check_call\u001B[39m(ret: \u001B[38;5;28mint\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    273\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Check the return value of C API call\u001B[39;00m\n\u001B[32m    274\u001B[39m \n\u001B[32m    275\u001B[39m \u001B[33;03m    This function will raise exception when error occurs.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    281\u001B[39m \u001B[33;03m        return value from API calls\u001B[39;00m\n\u001B[32m    282\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/core.py:222\u001B[39m, in \u001B[36m_load_lib\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    220\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lib_success:\n\u001B[32m    221\u001B[39m         libname = os.path.basename(lib_paths[\u001B[32m0\u001B[39m])\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m XGBoostError(\n\u001B[32m    223\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m    224\u001B[39m \u001B[33mXGBoost Library (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlibname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) could not be loaded.\u001B[39m\n\u001B[32m    225\u001B[39m \u001B[33mLikely causes:\u001B[39m\n\u001B[32m    226\u001B[39m \u001B[33m  * OpenMP runtime is not installed\u001B[39m\n\u001B[32m    227\u001B[39m \u001B[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001B[39m\n\u001B[32m    228\u001B[39m \u001B[33m    - libomp.dylib for Mac OSX\u001B[39m\n\u001B[32m    229\u001B[39m \u001B[33m    - libgomp.so for Linux and other UNIX-like OSes\u001B[39m\n\u001B[32m    230\u001B[39m \u001B[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001B[39m\n\u001B[32m    231\u001B[39m \n\u001B[32m    232\u001B[39m \u001B[33m  * You are running 32-bit Python on a 64-bit OS\u001B[39m\n\u001B[32m    233\u001B[39m \n\u001B[32m    234\u001B[39m \u001B[33mError message(s): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mos_error_list\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m    235\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m    236\u001B[39m         )\n\u001B[32m    237\u001B[39m     _register_log_callback(lib)\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mparse\u001B[39m(ver: \u001B[38;5;28mstr\u001B[39m) -> Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m]:\n",
      "\u001B[31mXGBoostError\u001B[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/maxmbey/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/maxmbey/Documents/Ecole/M1/Data/newRealDataMaster/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
